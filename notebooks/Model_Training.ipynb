{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths based on project structure\n",
    "data_path = \"../data/processed/\"\n",
    "models_path = \"../models/\"\n",
    "reports_path = \"../reports/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists before saving processed files\n",
    "import os\n",
    "\n",
    "data_path = \"../data/processed/\"\n",
    "os.makedirs(data_path, exist_ok=True)  # Creates the directory if it doesn't exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signup_time            int64\n",
      "purchase_time          int64\n",
      "purchase_value       float64\n",
      "source                 int64\n",
      "browser                int64\n",
      "age                    int64\n",
      "ip_address           float64\n",
      "class                  int64\n",
      "country                int64\n",
      "transaction_delay    float64\n",
      "hour_of_day            int64\n",
      "day_of_week            int64\n",
      "dtype: object\n",
      "âœ… Data Preparation Complete! Processed file saved as 'cleaned_fraud_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load Processed Fraud Data\n",
    "data_path = \"../data/processed_fraud_data.csv\"\n",
    "fraud_data = pd.read_csv(data_path)\n",
    "\n",
    "# ðŸ“Œ Drop Non-Numeric Columns\n",
    "drop_columns = [\"user_id\", \"device_id\", \"sex\"]  # Remove unnecessary text columns\n",
    "fraud_data = fraud_data.drop(columns=drop_columns)\n",
    "\n",
    "# ðŸ“Œ Convert Datetime Columns to Unix Timestamps\n",
    "datetime_columns = [\"signup_time\", \"purchase_time\"]\n",
    "for col in datetime_columns:\n",
    "    fraud_data[col] = pd.to_datetime(fraud_data[col], errors=\"coerce\").astype(int) // 10**9\n",
    "\n",
    "# ðŸ“Œ Verify That All Columns Are Numeric\n",
    "print(fraud_data.dtypes)  # Ensure all columns are int64 or float64\n",
    "\n",
    "# Save the cleaned dataset\n",
    "fraud_data.to_csv(\"../data/processed/cleaned_fraud_data.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Data Preparation Complete! Processed file saved as 'cleaned_fraud_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset\n",
    "fraud_data_path = \"../data/Fraud_Data.csv\"\n",
    "creditcard_data_path = \"../data/creditcard.csv\"\n",
    "\n",
    "fraud_data = pd.read_csv(fraud_data_path)\n",
    "creditcard_data = pd.read_csv(creditcard_data_path)\n",
    "\n",
    "# ðŸ“Œ Separate Features (X) and Target (y)\n",
    "X_fraud = fraud_data.drop(columns=[\"class\"])\n",
    "y_fraud = fraud_data[\"class\"]\n",
    "\n",
    "X_creditcard = creditcard_data.drop(columns=[\"Class\"])\n",
    "y_creditcard = creditcard_data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset\n",
    "fraud_data_path = \"../data/processed_fraud_data.csv\"\n",
    "creditcard_data_path = \"../data/creditcard.csv\"\n",
    "\n",
    "fraud_data = pd.read_csv(fraud_data_path)\n",
    "creditcard_data = pd.read_csv(creditcard_data_path)\n",
    "\n",
    "# ðŸ“Œ Separate Features (X) and Target (y)\n",
    "X_fraud = fraud_data.drop(columns=[\"class\"])\n",
    "y_fraud = fraud_data[\"class\"]\n",
    "\n",
    "X_creditcard = creditcard_data.drop(columns=[\"Class\"])\n",
    "y_creditcard = creditcard_data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Perform Train-Test Split\n",
    "X_fraud_train, X_fraud_test, y_fraud_train, y_fraud_test = train_test_split(\n",
    "    X_fraud, y_fraud, test_size=0.2, random_state=42, stratify=y_fraud\n",
    ")\n",
    "\n",
    "X_credit_train, X_credit_test, y_credit_train, y_credit_test = train_test_split(\n",
    "    X_creditcard, y_creditcard, test_size=0.2, random_state=42, stratify=y_creditcard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Encode Categorical Variables Using OrdinalEncoder\n",
    "categorical_columns = [\"browser\", \"source\", \"country\"]\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "X_fraud_train[categorical_columns] = ordinal_encoder.fit_transform(X_fraud_train[categorical_columns])\n",
    "X_fraud_test[categorical_columns] = ordinal_encoder.transform(X_fraud_test[categorical_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Normalize Numerical Features\n",
    "scaler = MinMaxScaler()\n",
    "numeric_columns = X_fraud_train.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "X_fraud_train_scaled = pd.DataFrame(scaler.fit_transform(X_fraud_train[numeric_columns]), columns=numeric_columns)\n",
    "X_fraud_test_scaled = pd.DataFrame(scaler.transform(X_fraud_test[numeric_columns]), columns=numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure categorical features are added back after scaling\n",
    "for col in categorical_columns:\n",
    "    X_fraud_train_scaled[col] = X_fraud_train[col].values\n",
    "    X_fraud_test_scaled[col] = X_fraud_test[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Save Processed Train-Test Datasets\n",
    "X_fraud_train_scaled.to_csv(os.path.join(data_path, \"X_fraud_train.csv\"), index=False)\n",
    "X_fraud_test_scaled.to_csv(os.path.join(data_path, \"X_fraud_test.csv\"), index=False)\n",
    "y_fraud_train.to_csv(os.path.join(data_path, \"y_fraud_train.csv\"), index=False)\n",
    "y_fraud_test.to_csv(os.path.join(data_path, \"y_fraud_test.csv\"), index=False)\n",
    "\n",
    "print(\" Data Preparation Completed: Processed files saved in '/data/processed/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Define Models for Fraud Detection\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Train and Evaluate Models\n",
    "model_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_fraud_train, y_fraud_train)\n",
    "    y_pred = model.predict(X_fraud_test)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(y_fraud_test, y_pred)\n",
    "    precision = precision_score(y_fraud_test, y_pred)\n",
    "    recall = recall_score(y_fraud_test, y_pred)\n",
    "    f1 = f1_score(y_fraud_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_fraud_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    model_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"ROC-AUC\": roc_auc\n",
    "    })\n",
    "\n",
    "    # Save the trained model\n",
    "    model_filename = f\"{name.replace(' ', '_')}.pkl\"\n",
    "    joblib.dump(model, os.path.join(models_path, model_filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-jkyell",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
